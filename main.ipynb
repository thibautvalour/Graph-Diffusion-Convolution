{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ab7Ps6ryXOGE",
        "outputId": "66b25d83-3c95-446d-a575-08304158133d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch has version 1.13.1+cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_scatter-2.1.0%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.0+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_sparse-0.6.16%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy->torch-sparse) (1.22.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.16+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.2.0.tar.gz (564 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.0/565.0 KB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (2.25.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.2.1)\n",
            "Collecting psutil>=5.8.0\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.2/280.2 KB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch-geometric) (2.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2022.12.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773302 sha256=4a94d69fc38649c031adf3db690662fc772d73f512b4dc901e62335b92e1dd67\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/b2/8c/9b4bb72a4384eabd1ffeab2b7ead692c9165e35711f8a9dc72\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: psutil, torch-geometric\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed psutil-5.9.4 torch-geometric-2.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 KB\u001b[0m \u001b[31m882.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.22.4)\n",
            "Collecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.13.1+cu116)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.26.14)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.2.1)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (4.65.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.3.5)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.9/dist-packages (from outdated>=0.2.0->ogb) (57.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from outdated>=0.2.0->ogb) (2.25.1)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->ogb) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->ogb) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->ogb) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->ogb) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7047 sha256=60baa0bbc8795380cfe99c560cf59658ea153df2223072b4b6e7d03c00c3bd4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/bb/0d/2d02ec45f29c48d6192476bfb59c5a0e64b605e7212374dd15\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.5 outdated-0.2.2\n",
            "Cloning into 'Graph-Diffusion-Convolution'...\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 36 (delta 18), reused 19 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (36/36), 16.34 KiB | 1.49 MiB/s, done.\n",
            "/content/Graph-Diffusion-Convolution\n"
          ]
        }
      ],
      "source": [
        "colab = True\n",
        "\n",
        "if colab:\n",
        "  import torch\n",
        "  import os\n",
        "  print(\"PyTorch has version {}\".format(torch.__version__))\n",
        "\n",
        "  # Install torch geometric\n",
        "  if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "    !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
        "    !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
        "    !pip install torch-geometric\n",
        "    !pip install ogb\n",
        "\n",
        "  !git clone https://github.com/thibautvalour/Graph-Diffusion-Convolution.git\n",
        "  %cd Graph-Diffusion-Convolution\n",
        "\n",
        "\n",
        "import os \n",
        "import math\n",
        "import torch\n",
        "from torch.nn.functional import nll_loss\n",
        "import copy\n",
        "\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "from models import GCN_Classifier\n",
        "from utils import train, test\n",
        "from matrix_format import compute_Tsym, gdc_pagerank\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWWuUkDqXNNU"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3pAL4mEXNNV",
        "outputId": "9267ded6-0fd5-4763-8d92-d26480e9a36e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.08 GB: 100%|██████████| 81/81 [00:09<00:00,  8.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2534.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 139.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  dataset_name = 'ogbn-arxiv'\n",
        "  dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                  transform=T.ToSparseTensor())\n",
        "  data = dataset[0]\n",
        "\n",
        "  # Make the adjacency matrix to symmetric\n",
        "  data.adj_t = data.adj_t.to_symmetric()\n",
        "  row, col, value = data.adj_t.coo()\n",
        "  value = torch.ones_like(row,  dtype=torch.float)\n",
        "\n",
        "  # Create a sparse tensor from the COO format\n",
        "  indices = torch.stack([row, col])\n",
        "  A = torch.sparse_coo_tensor(indices, value, \n",
        "                              size=[data.num_nodes, data.num_nodes]).to(device)\n",
        "\n",
        "  # If you use GPU, the device should be cuda\n",
        "  print('Device: {}'.format(device))\n",
        "\n",
        "  data = data.to(device)\n",
        "  split_idx = dataset.get_idx_split()\n",
        "  train_idx = split_idx['train'].to(device)\n",
        "  A = A.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKfVc6ZYXNNZ"
      },
      "source": [
        "# Define model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "btsATsIkXNNW"
      },
      "outputs": [],
      "source": [
        "Z = A"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCN"
      ],
      "metadata": {
        "id": "gTAyOM9gwjxG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1qEF-q-jXNNZ"
      },
      "outputs": [],
      "source": [
        "args = {\n",
        "    'device': device,\n",
        "    'num_layers': 5,\n",
        "    'hidden_dim': 264,\n",
        "    'dropout': 0.5,\n",
        "    'lr': 0.001,\n",
        "    'epochs': 100,\n",
        "    'trans_matrix': A\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_59aKjPXNNa",
        "outputId": "4e641880-d88e-41c5-fe4e-281fbccd3a0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/Graph-Diffusion-Convolution/models.py:45: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self.softmax(z3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 12.0960, Train: 13.01%, Valid: 26.32% Test: 25.57%\n",
            "Epoch: 02, Loss: 8.3025, Train: 13.33%, Valid: 23.96% Test: 22.86%\n",
            "Epoch: 03, Loss: 6.7655, Train: 15.11%, Valid: 30.90% Test: 33.33%\n",
            "Epoch: 04, Loss: 5.7117, Train: 10.63%, Valid: 16.19% Test: 22.99%\n",
            "Epoch: 05, Loss: 7.3076, Train: 21.63%, Valid: 25.40% Test: 24.55%\n",
            "Epoch: 06, Loss: 5.2429, Train: 27.78%, Valid: 35.09% Test: 33.23%\n",
            "Epoch: 07, Loss: 5.2453, Train: 27.80%, Valid: 32.54% Test: 30.18%\n",
            "Epoch: 08, Loss: 5.7913, Train: 28.69%, Valid: 33.75% Test: 31.26%\n",
            "Epoch: 09, Loss: 5.7116, Train: 29.32%, Valid: 35.85% Test: 33.57%\n",
            "Epoch: 10, Loss: 5.0368, Train: 29.37%, Valid: 36.45% Test: 34.00%\n",
            "Epoch: 11, Loss: 5.0247, Train: 27.63%, Valid: 31.46% Test: 28.64%\n",
            "Epoch: 12, Loss: 5.0929, Train: 23.22%, Valid: 19.99% Test: 17.67%\n",
            "Epoch: 13, Loss: 5.3382, Train: 22.06%, Valid: 17.93% Test: 16.16%\n",
            "Epoch: 14, Loss: 5.5351, Train: 26.09%, Valid: 30.03% Test: 34.38%\n",
            "Epoch: 15, Loss: 4.7173, Train: 26.57%, Valid: 32.05% Test: 35.40%\n",
            "Epoch: 16, Loss: 4.2698, Train: 27.57%, Valid: 34.86% Test: 37.22%\n",
            "Epoch: 17, Loss: 4.5202, Train: 28.00%, Valid: 35.23% Test: 37.06%\n",
            "Epoch: 18, Loss: 4.6860, Train: 27.34%, Valid: 34.48% Test: 37.11%\n",
            "Epoch: 19, Loss: 4.5953, Train: 26.01%, Valid: 31.42% Test: 34.95%\n",
            "Epoch: 20, Loss: 4.4979, Train: 24.56%, Valid: 26.95% Test: 31.19%\n",
            "Epoch: 21, Loss: 4.9029, Train: 24.08%, Valid: 26.32% Test: 31.42%\n",
            "Epoch: 22, Loss: 4.8397, Train: 24.19%, Valid: 26.44% Test: 31.21%\n",
            "Epoch: 23, Loss: 4.6394, Train: 24.30%, Valid: 26.74% Test: 31.23%\n",
            "Epoch: 24, Loss: 4.5356, Train: 24.08%, Valid: 26.14% Test: 30.89%\n",
            "Epoch: 25, Loss: 4.3619, Train: 23.61%, Valid: 25.07% Test: 30.23%\n",
            "Epoch: 26, Loss: 4.3582, Train: 23.56%, Valid: 25.01% Test: 30.05%\n",
            "Epoch: 27, Loss: 4.5248, Train: 23.65%, Valid: 25.15% Test: 29.98%\n",
            "Epoch: 28, Loss: 5.2690, Train: 27.38%, Valid: 34.82% Test: 38.96%\n",
            "Epoch: 29, Loss: 4.5158, Train: 30.39%, Valid: 40.99% Test: 42.13%\n",
            "Epoch: 30, Loss: 4.3093, Train: 30.52%, Valid: 40.31% Test: 39.60%\n",
            "Epoch: 31, Loss: 4.2705, Train: 30.57%, Valid: 39.94% Test: 39.15%\n",
            "Epoch: 32, Loss: 4.1893, Train: 31.40%, Valid: 39.46% Test: 38.29%\n",
            "Epoch: 33, Loss: 4.2461, Train: 30.19%, Valid: 38.68% Test: 38.09%\n",
            "Epoch: 34, Loss: 4.0509, Train: 28.73%, Valid: 36.88% Test: 36.93%\n",
            "Epoch: 35, Loss: 3.9622, Train: 26.67%, Valid: 31.00% Test: 32.58%\n",
            "Epoch: 36, Loss: 3.8464, Train: 24.28%, Valid: 25.13% Test: 29.10%\n",
            "Epoch: 37, Loss: 4.1774, Train: 23.70%, Valid: 24.16% Test: 26.40%\n",
            "Epoch: 38, Loss: 4.1545, Train: 24.29%, Valid: 24.22% Test: 24.04%\n",
            "Epoch: 39, Loss: 4.1496, Train: 27.86%, Valid: 35.61% Test: 37.52%\n",
            "Epoch: 40, Loss: 4.0344, Train: 28.75%, Valid: 37.70% Test: 39.77%\n",
            "Epoch: 41, Loss: 3.9159, Train: 29.75%, Valid: 40.09% Test: 40.82%\n",
            "Epoch: 42, Loss: 4.0080, Train: 29.84%, Valid: 40.29% Test: 40.90%\n",
            "Epoch: 43, Loss: 3.8279, Train: 29.98%, Valid: 40.49% Test: 41.44%\n",
            "Epoch: 44, Loss: 3.8753, Train: 30.22%, Valid: 40.69% Test: 42.72%\n",
            "Epoch: 45, Loss: 3.7885, Train: 28.94%, Valid: 38.21% Test: 41.60%\n",
            "Epoch: 46, Loss: 4.0979, Train: 27.54%, Valid: 34.93% Test: 38.71%\n",
            "Epoch: 47, Loss: 4.4670, Train: 29.43%, Valid: 39.71% Test: 41.80%\n",
            "Epoch: 48, Loss: 3.9586, Train: 29.21%, Valid: 38.67% Test: 39.36%\n",
            "Epoch: 49, Loss: 3.9025, Train: 28.84%, Valid: 37.60% Test: 37.55%\n",
            "Epoch: 50, Loss: 3.8324, Train: 28.75%, Valid: 37.26% Test: 37.03%\n",
            "Epoch: 51, Loss: 4.2046, Train: 28.91%, Valid: 37.39% Test: 37.48%\n",
            "Epoch: 52, Loss: 3.9199, Train: 29.01%, Valid: 37.61% Test: 38.77%\n",
            "Epoch: 53, Loss: 3.8199, Train: 28.79%, Valid: 37.14% Test: 39.47%\n",
            "Epoch: 54, Loss: 3.7954, Train: 28.00%, Valid: 34.77% Test: 37.44%\n",
            "Epoch: 55, Loss: 4.6976, Train: 31.04%, Valid: 39.56% Test: 41.36%\n",
            "Epoch: 56, Loss: 4.0436, Train: 30.18%, Valid: 37.95% Test: 37.85%\n",
            "Epoch: 57, Loss: 3.7761, Train: 29.04%, Valid: 34.43% Test: 33.03%\n",
            "Epoch: 58, Loss: 3.5867, Train: 29.15%, Valid: 34.46% Test: 32.30%\n",
            "Epoch: 59, Loss: 3.7479, Train: 28.85%, Valid: 34.67% Test: 32.44%\n",
            "Epoch: 60, Loss: 3.7410, Train: 28.24%, Valid: 34.36% Test: 32.51%\n",
            "Epoch: 61, Loss: 3.6846, Train: 27.40%, Valid: 32.53% Test: 30.78%\n",
            "Epoch: 62, Loss: 3.9005, Train: 28.16%, Valid: 34.60% Test: 33.31%\n",
            "Epoch: 63, Loss: 3.6876, Train: 28.63%, Valid: 36.41% Test: 36.54%\n",
            "Epoch: 64, Loss: 4.0124, Train: 28.98%, Valid: 37.26% Test: 37.16%\n",
            "Epoch: 65, Loss: 3.7726, Train: 29.41%, Valid: 38.43% Test: 38.38%\n",
            "Epoch: 66, Loss: 3.9863, Train: 30.00%, Valid: 40.01% Test: 40.99%\n",
            "Epoch: 67, Loss: 3.9251, Train: 30.64%, Valid: 40.85% Test: 43.49%\n",
            "Epoch: 68, Loss: 3.6026, Train: 30.69%, Valid: 38.85% Test: 42.28%\n",
            "Epoch: 69, Loss: 3.8184, Train: 32.75%, Valid: 42.29% Test: 44.80%\n",
            "Epoch: 70, Loss: 3.6790, Train: 32.30%, Valid: 43.35% Test: 45.19%\n",
            "Epoch: 71, Loss: 3.5165, Train: 31.36%, Valid: 42.76% Test: 44.95%\n",
            "Epoch: 72, Loss: 3.6045, Train: 30.80%, Valid: 42.45% Test: 44.64%\n",
            "Epoch: 73, Loss: 3.7914, Train: 31.50%, Valid: 42.71% Test: 44.88%\n",
            "Epoch: 74, Loss: 3.7300, Train: 32.78%, Valid: 43.66% Test: 44.74%\n",
            "Epoch: 75, Loss: 3.6838, Train: 33.13%, Valid: 43.01% Test: 43.13%\n",
            "Epoch: 76, Loss: 3.9583, Train: 32.48%, Valid: 40.34% Test: 39.84%\n",
            "Epoch: 77, Loss: 3.5793, Train: 31.25%, Valid: 36.58% Test: 36.06%\n",
            "Epoch: 78, Loss: 3.6379, Train: 29.64%, Valid: 32.14% Test: 31.84%\n",
            "Epoch: 79, Loss: 3.5219, Train: 28.29%, Valid: 29.75% Test: 31.13%\n",
            "Epoch: 80, Loss: 3.9154, Train: 30.22%, Valid: 34.97% Test: 37.10%\n",
            "Epoch: 81, Loss: 3.5720, Train: 32.24%, Valid: 41.02% Test: 42.73%\n",
            "Epoch: 82, Loss: 3.7553, Train: 32.49%, Valid: 42.79% Test: 44.16%\n",
            "Epoch: 83, Loss: 3.5282, Train: 32.59%, Valid: 43.05% Test: 44.21%\n",
            "Epoch: 84, Loss: 3.6522, Train: 32.56%, Valid: 42.59% Test: 43.25%\n",
            "Epoch: 85, Loss: 3.5555, Train: 33.16%, Valid: 42.63% Test: 42.82%\n",
            "Epoch: 86, Loss: 3.6053, Train: 34.39%, Valid: 43.01% Test: 43.14%\n",
            "Epoch: 87, Loss: 3.4525, Train: 33.65%, Valid: 40.72% Test: 42.23%\n",
            "Epoch: 88, Loss: 3.5142, Train: 31.51%, Valid: 36.63% Test: 39.15%\n",
            "Epoch: 89, Loss: 3.5179, Train: 29.78%, Valid: 34.07% Test: 37.32%\n",
            "Epoch: 90, Loss: 4.2544, Train: 32.32%, Valid: 41.82% Test: 43.57%\n",
            "Epoch: 91, Loss: 3.4953, Train: 31.55%, Valid: 40.76% Test: 41.48%\n",
            "Epoch: 92, Loss: 3.6230, Train: 30.77%, Valid: 38.98% Test: 38.93%\n",
            "Epoch: 93, Loss: 3.5978, Train: 30.49%, Valid: 38.07% Test: 38.21%\n",
            "Epoch: 94, Loss: 3.7419, Train: 30.99%, Valid: 38.84% Test: 39.45%\n",
            "Epoch: 95, Loss: 3.7193, Train: 31.21%, Valid: 39.28% Test: 39.53%\n",
            "Epoch: 96, Loss: 3.4541, Train: 31.46%, Valid: 39.71% Test: 40.04%\n",
            "Epoch: 97, Loss: 3.5886, Train: 32.47%, Valid: 40.53% Test: 40.51%\n",
            "Epoch: 98, Loss: 3.7845, Train: 34.28%, Valid: 41.83% Test: 42.19%\n",
            "Epoch: 99, Loss: 3.6374, Train: 34.66%, Valid: 42.13% Test: 43.34%\n",
            "Epoch: 100, Loss: 3.5826, Train: 32.96%, Valid: 37.70% Test: 40.31%\n"
          ]
        }
      ],
      "source": [
        "model = GCN_Classifier(input_dim=dataset.num_features,\n",
        "                       hidden_dim=args['hidden_dim'],\n",
        "                       output_dim=dataset.num_classes,\n",
        "                       dropout=args['dropout']).to(args['device'])\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "evaluator = Evaluator(name='ogbn-arxiv')\n",
        "loss_fn = nll_loss\n",
        "\n",
        "best_model = None\n",
        "best_valid_acc = 0\n",
        "\n",
        "for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "  loss = train(model, data, train_idx, args['trans_matrix'], optimizer, loss_fn)\n",
        "  result = test(model, data, split_idx, args['trans_matrix'], evaluator)\n",
        "  train_acc, valid_acc, test_acc = result\n",
        "  if valid_acc > best_valid_acc:\n",
        "      best_valid_acc = valid_acc\n",
        "      best_model = copy.deepcopy(model)\n",
        "  print(f'Epoch: {epoch:02d}, '\n",
        "        f'Loss: {loss:.4f}, '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GDN CLASSIQUE"
      ],
      "metadata": {
        "id": "vcBQQg2rwrh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T_sym = compute_Tsym(A)"
      ],
      "metadata": {
        "id": "l8i1dP06w8pX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "v940A4AdXNNb"
      },
      "outputs": [],
      "source": [
        "args = {\n",
        "    'device': device,\n",
        "    'num_layers': 5,\n",
        "    'hidden_dim': 64,\n",
        "    'dropout': 0.5,\n",
        "    'lr': 0.001,\n",
        "    'epochs': 100,\n",
        "    'trans_matrix': T_sym\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bWqaZyPXNNb",
        "outputId": "c6fe8b17-8fed-4eea-8913-8c24bdcdc136"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 3.7316, Train: 0.96%, Valid: 1.00% Test: 1.28%\n",
            "Epoch: 02, Loss: 3.6392, Train: 1.42%, Valid: 1.19% Test: 1.31%\n",
            "Epoch: 03, Loss: 3.5692, Train: 2.48%, Valid: 2.19% Test: 1.49%\n",
            "Epoch: 04, Loss: 3.4977, Train: 3.80%, Valid: 4.32% Test: 2.55%\n",
            "Epoch: 05, Loss: 3.4320, Train: 5.14%, Valid: 7.38% Test: 6.02%\n",
            "Epoch: 06, Loss: 3.3721, Train: 6.78%, Valid: 10.97% Test: 10.49%\n",
            "Epoch: 07, Loss: 3.3145, Train: 9.13%, Valid: 14.17% Test: 14.08%\n",
            "Epoch: 08, Loss: 3.2548, Train: 11.29%, Valid: 15.57% Test: 14.90%\n",
            "Epoch: 09, Loss: 3.2054, Train: 12.75%, Valid: 16.22% Test: 15.12%\n",
            "Epoch: 10, Loss: 3.1543, Train: 13.90%, Valid: 16.73% Test: 15.52%\n",
            "Epoch: 11, Loss: 3.1127, Train: 14.96%, Valid: 17.21% Test: 15.78%\n",
            "Epoch: 12, Loss: 3.0693, Train: 16.20%, Valid: 17.94% Test: 16.28%\n",
            "Epoch: 13, Loss: 3.0247, Train: 17.60%, Valid: 18.70% Test: 16.98%\n",
            "Epoch: 14, Loss: 2.9853, Train: 19.04%, Valid: 19.64% Test: 17.85%\n",
            "Epoch: 15, Loss: 2.9503, Train: 20.67%, Valid: 21.13% Test: 19.37%\n",
            "Epoch: 16, Loss: 2.9141, Train: 22.48%, Valid: 22.87% Test: 21.05%\n",
            "Epoch: 17, Loss: 2.8763, Train: 24.19%, Valid: 24.93% Test: 22.98%\n",
            "Epoch: 18, Loss: 2.8414, Train: 25.93%, Valid: 27.15% Test: 24.70%\n",
            "Epoch: 19, Loss: 2.8121, Train: 27.48%, Valid: 28.84% Test: 26.10%\n",
            "Epoch: 20, Loss: 2.7799, Train: 28.82%, Valid: 30.12% Test: 27.00%\n",
            "Epoch: 21, Loss: 2.7446, Train: 29.79%, Valid: 30.95% Test: 27.68%\n",
            "Epoch: 22, Loss: 2.7189, Train: 30.58%, Valid: 31.68% Test: 28.18%\n",
            "Epoch: 23, Loss: 2.6865, Train: 31.03%, Valid: 32.19% Test: 28.59%\n",
            "Epoch: 24, Loss: 2.6604, Train: 31.27%, Valid: 32.36% Test: 28.89%\n",
            "Epoch: 25, Loss: 2.6342, Train: 31.50%, Valid: 32.58% Test: 29.08%\n",
            "Epoch: 26, Loss: 2.6067, Train: 31.76%, Valid: 32.75% Test: 29.35%\n",
            "Epoch: 27, Loss: 2.5772, Train: 32.20%, Valid: 33.10% Test: 29.63%\n",
            "Epoch: 28, Loss: 2.5516, Train: 32.79%, Valid: 33.45% Test: 29.99%\n",
            "Epoch: 29, Loss: 2.5257, Train: 33.73%, Valid: 34.05% Test: 30.52%\n",
            "Epoch: 30, Loss: 2.4988, Train: 34.78%, Valid: 34.73% Test: 31.14%\n",
            "Epoch: 31, Loss: 2.4766, Train: 35.98%, Valid: 35.56% Test: 31.82%\n",
            "Epoch: 32, Loss: 2.4584, Train: 37.21%, Valid: 36.31% Test: 32.52%\n",
            "Epoch: 33, Loss: 2.4324, Train: 38.31%, Valid: 37.23% Test: 33.18%\n",
            "Epoch: 34, Loss: 2.4135, Train: 39.19%, Valid: 37.91% Test: 33.82%\n",
            "Epoch: 35, Loss: 2.3905, Train: 39.98%, Valid: 38.54% Test: 34.36%\n",
            "Epoch: 36, Loss: 2.3728, Train: 40.69%, Valid: 39.24% Test: 35.01%\n",
            "Epoch: 37, Loss: 2.3470, Train: 41.32%, Valid: 39.90% Test: 35.61%\n",
            "Epoch: 38, Loss: 2.3278, Train: 41.95%, Valid: 40.42% Test: 36.19%\n",
            "Epoch: 39, Loss: 2.3165, Train: 42.44%, Valid: 40.74% Test: 36.57%\n",
            "Epoch: 40, Loss: 2.2941, Train: 42.94%, Valid: 41.03% Test: 36.78%\n",
            "Epoch: 41, Loss: 2.2772, Train: 43.45%, Valid: 41.35% Test: 37.08%\n",
            "Epoch: 42, Loss: 2.2578, Train: 44.04%, Valid: 41.70% Test: 37.46%\n",
            "Epoch: 43, Loss: 2.2435, Train: 44.79%, Valid: 42.30% Test: 37.97%\n",
            "Epoch: 44, Loss: 2.2273, Train: 45.54%, Valid: 43.00% Test: 38.68%\n",
            "Epoch: 45, Loss: 2.2105, Train: 46.40%, Valid: 43.78% Test: 39.52%\n",
            "Epoch: 46, Loss: 2.1943, Train: 47.27%, Valid: 44.73% Test: 40.39%\n",
            "Epoch: 47, Loss: 2.1802, Train: 48.03%, Valid: 45.57% Test: 41.23%\n",
            "Epoch: 48, Loss: 2.1624, Train: 48.69%, Valid: 46.26% Test: 42.03%\n",
            "Epoch: 49, Loss: 2.1485, Train: 49.31%, Valid: 46.85% Test: 42.71%\n",
            "Epoch: 50, Loss: 2.1370, Train: 49.80%, Valid: 47.37% Test: 43.33%\n",
            "Epoch: 51, Loss: 2.1252, Train: 50.25%, Valid: 47.77% Test: 43.90%\n",
            "Epoch: 52, Loss: 2.1047, Train: 50.64%, Valid: 48.21% Test: 44.41%\n",
            "Epoch: 53, Loss: 2.0966, Train: 51.04%, Valid: 48.65% Test: 44.93%\n",
            "Epoch: 54, Loss: 2.0828, Train: 51.54%, Valid: 49.20% Test: 45.61%\n",
            "Epoch: 55, Loss: 2.0680, Train: 52.06%, Valid: 49.79% Test: 46.32%\n",
            "Epoch: 56, Loss: 2.0592, Train: 52.60%, Valid: 50.37% Test: 47.06%\n",
            "Epoch: 57, Loss: 2.0486, Train: 53.19%, Valid: 50.98% Test: 47.79%\n",
            "Epoch: 58, Loss: 2.0330, Train: 53.67%, Valid: 51.53% Test: 48.62%\n",
            "Epoch: 59, Loss: 2.0206, Train: 54.17%, Valid: 52.41% Test: 49.44%\n",
            "Epoch: 60, Loss: 2.0148, Train: 54.59%, Valid: 53.03% Test: 50.13%\n",
            "Epoch: 61, Loss: 1.9947, Train: 54.93%, Valid: 53.54% Test: 50.77%\n",
            "Epoch: 62, Loss: 1.9907, Train: 55.23%, Valid: 53.87% Test: 51.19%\n",
            "Epoch: 63, Loss: 1.9771, Train: 55.52%, Valid: 54.23% Test: 51.52%\n",
            "Epoch: 64, Loss: 1.9689, Train: 55.72%, Valid: 54.39% Test: 51.70%\n",
            "Epoch: 65, Loss: 1.9606, Train: 55.93%, Valid: 54.54% Test: 51.80%\n",
            "Epoch: 66, Loss: 1.9513, Train: 56.08%, Valid: 54.57% Test: 51.89%\n",
            "Epoch: 67, Loss: 1.9373, Train: 56.27%, Valid: 54.70% Test: 52.09%\n",
            "Epoch: 68, Loss: 1.9298, Train: 56.49%, Valid: 55.00% Test: 52.41%\n",
            "Epoch: 69, Loss: 1.9215, Train: 56.79%, Valid: 55.40% Test: 53.01%\n",
            "Epoch: 70, Loss: 1.9042, Train: 57.12%, Valid: 55.84% Test: 53.63%\n",
            "Epoch: 71, Loss: 1.8994, Train: 57.46%, Valid: 56.36% Test: 54.21%\n",
            "Epoch: 72, Loss: 1.8906, Train: 57.72%, Valid: 56.78% Test: 54.86%\n",
            "Epoch: 73, Loss: 1.8838, Train: 57.93%, Valid: 57.16% Test: 55.38%\n",
            "Epoch: 74, Loss: 1.8757, Train: 58.09%, Valid: 57.39% Test: 55.63%\n",
            "Epoch: 75, Loss: 1.8662, Train: 58.24%, Valid: 57.51% Test: 55.73%\n",
            "Epoch: 76, Loss: 1.8602, Train: 58.35%, Valid: 57.40% Test: 55.63%\n",
            "Epoch: 77, Loss: 1.8494, Train: 58.48%, Valid: 57.38% Test: 55.40%\n",
            "Epoch: 78, Loss: 1.8435, Train: 58.60%, Valid: 57.40% Test: 55.24%\n",
            "Epoch: 79, Loss: 1.8303, Train: 58.74%, Valid: 57.41% Test: 55.16%\n",
            "Epoch: 80, Loss: 1.8235, Train: 58.89%, Valid: 57.51% Test: 55.24%\n",
            "Epoch: 81, Loss: 1.8144, Train: 59.07%, Valid: 57.65% Test: 55.37%\n",
            "Epoch: 82, Loss: 1.8094, Train: 59.26%, Valid: 57.86% Test: 55.57%\n",
            "Epoch: 83, Loss: 1.7987, Train: 59.49%, Valid: 58.19% Test: 55.81%\n",
            "Epoch: 84, Loss: 1.7955, Train: 59.73%, Valid: 58.54% Test: 56.25%\n",
            "Epoch: 85, Loss: 1.7892, Train: 60.02%, Valid: 58.91% Test: 56.65%\n",
            "Epoch: 86, Loss: 1.7793, Train: 60.20%, Valid: 59.17% Test: 56.91%\n",
            "Epoch: 87, Loss: 1.7730, Train: 60.41%, Valid: 59.37% Test: 57.23%\n",
            "Epoch: 88, Loss: 1.7674, Train: 60.50%, Valid: 59.44% Test: 57.32%\n",
            "Epoch: 89, Loss: 1.7620, Train: 60.62%, Valid: 59.49% Test: 57.37%\n",
            "Epoch: 90, Loss: 1.7550, Train: 60.71%, Valid: 59.57% Test: 57.44%\n",
            "Epoch: 91, Loss: 1.7482, Train: 60.81%, Valid: 59.62% Test: 57.52%\n",
            "Epoch: 92, Loss: 1.7404, Train: 60.95%, Valid: 59.68% Test: 57.60%\n",
            "Epoch: 93, Loss: 1.7340, Train: 61.02%, Valid: 59.75% Test: 57.72%\n",
            "Epoch: 94, Loss: 1.7319, Train: 61.14%, Valid: 59.91% Test: 57.97%\n",
            "Epoch: 95, Loss: 1.7217, Train: 61.34%, Valid: 60.18% Test: 58.30%\n",
            "Epoch: 96, Loss: 1.7148, Train: 61.44%, Valid: 60.53% Test: 58.62%\n",
            "Epoch: 97, Loss: 1.7076, Train: 61.63%, Valid: 61.02% Test: 59.03%\n",
            "Epoch: 98, Loss: 1.7081, Train: 61.77%, Valid: 61.25% Test: 59.34%\n",
            "Epoch: 99, Loss: 1.6992, Train: 61.88%, Valid: 61.40% Test: 59.49%\n",
            "Epoch: 100, Loss: 1.6930, Train: 61.96%, Valid: 61.50% Test: 59.65%\n"
          ]
        }
      ],
      "source": [
        "model = GCN_Classifier(input_dim=dataset.num_features,\n",
        "                       hidden_dim=args['hidden_dim'],\n",
        "                       output_dim=dataset.num_classes,\n",
        "                       dropout=args['dropout']).to(args['device'])\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "evaluator = Evaluator(name='ogbn-arxiv')\n",
        "loss_fn = nll_loss\n",
        "\n",
        "best_model = None\n",
        "best_valid_acc = 0\n",
        "\n",
        "for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "  loss = train(model, data, train_idx, args['trans_matrix'], optimizer, loss_fn)\n",
        "  result = test(model, data, split_idx, args['trans_matrix'], evaluator)\n",
        "  train_acc, valid_acc, test_acc = result\n",
        "  if valid_acc > best_valid_acc:\n",
        "      best_valid_acc = valid_acc\n",
        "      best_model = copy.deepcopy(model)\n",
        "  print(f'Epoch: {epoch:02d}, '\n",
        "        f'Loss: {loss:.4f}, '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFL6fcGFdoKc"
      },
      "source": [
        "# Diffusion pagerank (ppr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt,scipy as sp\n",
        "import numpy as np\n",
        "from torch.linalg import inv\n",
        "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def compute_Tsym(adj): \n",
        "    ''' adj is a sparse matrix'''\n",
        "    N = adj.shape[0]\n",
        "    D = torch.sparse.sum(adj, dim=1).to_dense() # get degree matrix D\n",
        "    D_sqrt_inv = torch.pow(D, -0.5)\n",
        "    indices = torch.arange(N).unsqueeze(0).repeat(2, 1).to(device)\n",
        "    D_sqrt_inv = torch.sparse_coo_tensor(indices, D_sqrt_inv,\n",
        "                                         size=(N, N))\n",
        "    \n",
        "    Tsym = D_sqrt_inv.matmul(adj).matmul(D_sqrt_inv)\n",
        "    return Tsym\n",
        "\n",
        "def gdc_pagerank2(A, alpha, eps):\n",
        "    \n",
        "    N = A.shape[0]\n",
        "\n",
        "    # Self-loops\n",
        "    indices = torch.arange(N).unsqueeze(0).repeat(2, 1).to(device) \n",
        "    values = torch.ones(N, dtype=torch.float).to(device) \n",
        "    sparse_identity = torch.sparse_coo_tensor(indices, values,\n",
        "                                             size=(N, N))    \n",
        "    \n",
        "    A_loop = A + sparse_identity\n",
        "    \n",
        "    # Symmetric transition matrix\n",
        "    D_loop = torch.sparse.sum(A_loop, dim=1).to_dense()\n",
        "    D_sqrt_inv = torch.pow(D_loop, -0.5)\n",
        "    D_sqrt_inv = torch.sparse_coo_tensor(indices, D_sqrt_inv,\n",
        "                                         size=(N, N))\n",
        "\n",
        "\n",
        "    T_sym = D_sqrt_inv @ A_loop @ D_sqrt_inv\n",
        "\n",
        "    # PPR-based diffusion\n",
        "    test = (sparse_identity-(1-alpha)*T_sym).to_dense()\n",
        "    print(type(test))\n",
        "    #test2 = inv(test)\n",
        "    \n",
        "    S = alpha * torch.pow(sparse_identity-(1-alpha)*T_sym,-1)\n",
        "\n",
        "    # TODO : check why negative values are present in S\n",
        "    # Sparsify using threshold epsilon\n",
        "    indices = S.indices()\n",
        "    thresholded_val = S.values() * (S.values() >= eps)\n",
        "    S_tilde = torch.sparse_coo_tensor(indices, thresholded_val,\n",
        "                                      size=(N, N))\n",
        "\n",
        "    # Column-normalized transition matrix on graph S_tilde\n",
        "    D_tilde_vec = torch.sparse.sum(S_tilde, dim=1).to_dense()\n",
        "    indices = torch.arange(N).unsqueeze(0).repeat(2, 1).to(device) \n",
        "    D_tilde_vec = torch.sparse_coo_tensor(indices, D_tilde_vec,\n",
        "                                            size=(N, N))\n",
        "    T_S = S_tilde @ torch.pow(D_tilde_vec, -1)\n",
        "    \n",
        "    print(\"yes\")\n",
        "    \n",
        "    return T_S\n",
        "\n"
      ],
      "metadata": {
        "id": "VQDVSx64IHwh"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "19991BwmXNNY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "4ae803dc-aca3-4511-dbeb-a29e90ca28e3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-b48cf0108975>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#from matrix_format import compute_Tsym, gdc_pagerank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mS_pr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgdc_pagerank2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-84-5668ab46dd38>\u001b[0m in \u001b[0;36mgdc_pagerank2\u001b[0;34m(A, alpha, eps)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# PPR-based diffusion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msparse_identity\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mT_sym\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m#test2 = inv(test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 14.75 GiB total capacity; 1006.36 MiB already allocated; 12.58 GiB free; 1.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "#from matrix_format import compute_Tsym, gdc_pagerank\n",
        "S_pr = gdc_pagerank2(A, 0.05, 1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pOPyQai0XNNb"
      },
      "outputs": [],
      "source": [
        "args = {\n",
        "    'device': device,\n",
        "    'num_layers': 5,\n",
        "    'hidden_dim': 64,\n",
        "    'dropout': 0.5,\n",
        "    'lr': 0.001,\n",
        "    'epochs': 100,\n",
        "    'trans_matrix': S_pr\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQs019BUXNNc",
        "outputId": "c59f5a70-aa9b-4ebf-a521-4a07239d6097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 3.8805, Train: 2.69%, Valid: 1.76% Test: 1.48%\n",
            "Epoch: 02, Loss: 3.8244, Train: 6.35%, Valid: 10.36% Test: 9.36%\n",
            "Epoch: 03, Loss: 3.7764, Train: 10.59%, Valid: 20.33% Test: 18.79%\n",
            "Epoch: 04, Loss: 3.7262, Train: 12.16%, Valid: 23.04% Test: 21.27%\n",
            "Epoch: 05, Loss: 3.6777, Train: 13.72%, Valid: 24.33% Test: 22.32%\n",
            "Epoch: 06, Loss: 3.6278, Train: 14.63%, Valid: 24.51% Test: 22.37%\n",
            "Epoch: 07, Loss: 3.5808, Train: 14.76%, Valid: 23.91% Test: 21.71%\n",
            "Epoch: 08, Loss: 3.5353, Train: 14.77%, Valid: 22.99% Test: 20.79%\n",
            "Epoch: 09, Loss: 3.4911, Train: 14.81%, Valid: 22.35% Test: 20.04%\n",
            "Epoch: 10, Loss: 3.4507, Train: 15.16%, Valid: 22.05% Test: 19.56%\n",
            "Epoch: 11, Loss: 3.4085, Train: 15.85%, Valid: 21.97% Test: 19.49%\n",
            "Epoch: 12, Loss: 3.3658, Train: 16.83%, Valid: 22.16% Test: 19.68%\n",
            "Epoch: 13, Loss: 3.3234, Train: 18.09%, Valid: 22.68% Test: 20.04%\n",
            "Epoch: 14, Loss: 3.2879, Train: 19.43%, Valid: 23.43% Test: 20.59%\n",
            "Epoch: 15, Loss: 3.2490, Train: 20.86%, Valid: 24.24% Test: 21.29%\n",
            "Epoch: 16, Loss: 3.2116, Train: 22.28%, Valid: 25.03% Test: 22.00%\n",
            "Epoch: 17, Loss: 3.1815, Train: 23.61%, Valid: 25.88% Test: 22.75%\n",
            "Epoch: 18, Loss: 3.1434, Train: 24.77%, Valid: 26.75% Test: 23.49%\n",
            "Epoch: 19, Loss: 3.1114, Train: 25.76%, Valid: 27.50% Test: 24.18%\n",
            "Epoch: 20, Loss: 3.0835, Train: 26.62%, Valid: 28.23% Test: 24.81%\n",
            "Epoch: 21, Loss: 3.0551, Train: 27.39%, Valid: 29.02% Test: 25.57%\n",
            "Epoch: 22, Loss: 3.0236, Train: 28.13%, Valid: 29.65% Test: 26.20%\n",
            "Epoch: 23, Loss: 2.9977, Train: 28.76%, Valid: 30.29% Test: 26.97%\n",
            "Epoch: 24, Loss: 2.9694, Train: 29.36%, Valid: 30.93% Test: 27.64%\n",
            "Epoch: 25, Loss: 2.9472, Train: 29.98%, Valid: 31.54% Test: 28.32%\n",
            "Epoch: 26, Loss: 2.9210, Train: 30.50%, Valid: 32.22% Test: 28.94%\n",
            "Epoch: 27, Loss: 2.8993, Train: 31.02%, Valid: 32.75% Test: 29.73%\n",
            "Epoch: 28, Loss: 2.8797, Train: 31.55%, Valid: 33.30% Test: 30.45%\n",
            "Epoch: 29, Loss: 2.8595, Train: 32.11%, Valid: 33.94% Test: 31.21%\n",
            "Epoch: 30, Loss: 2.8366, Train: 32.64%, Valid: 34.55% Test: 31.91%\n",
            "Epoch: 31, Loss: 2.8209, Train: 33.17%, Valid: 35.21% Test: 32.59%\n",
            "Epoch: 32, Loss: 2.8045, Train: 33.69%, Valid: 35.87% Test: 33.34%\n",
            "Epoch: 33, Loss: 2.7873, Train: 34.20%, Valid: 36.46% Test: 34.09%\n",
            "Epoch: 34, Loss: 2.7693, Train: 34.69%, Valid: 37.01% Test: 34.81%\n",
            "Epoch: 35, Loss: 2.7549, Train: 35.15%, Valid: 37.51% Test: 35.51%\n",
            "Epoch: 36, Loss: 2.7406, Train: 35.56%, Valid: 38.08% Test: 36.14%\n",
            "Epoch: 37, Loss: 2.7252, Train: 36.04%, Valid: 38.64% Test: 36.70%\n",
            "Epoch: 38, Loss: 2.7120, Train: 36.45%, Valid: 39.12% Test: 37.27%\n",
            "Epoch: 39, Loss: 2.6974, Train: 36.83%, Valid: 39.61% Test: 37.74%\n",
            "Epoch: 40, Loss: 2.6829, Train: 37.22%, Valid: 40.01% Test: 38.15%\n",
            "Epoch: 41, Loss: 2.6734, Train: 37.55%, Valid: 40.44% Test: 38.61%\n",
            "Epoch: 42, Loss: 2.6581, Train: 37.87%, Valid: 40.85% Test: 39.03%\n",
            "Epoch: 43, Loss: 2.6483, Train: 38.18%, Valid: 41.20% Test: 39.43%\n",
            "Epoch: 44, Loss: 2.6337, Train: 38.48%, Valid: 41.48% Test: 39.81%\n",
            "Epoch: 45, Loss: 2.6250, Train: 38.76%, Valid: 41.70% Test: 40.13%\n",
            "Epoch: 46, Loss: 2.6101, Train: 39.01%, Valid: 41.97% Test: 40.38%\n",
            "Epoch: 47, Loss: 2.6022, Train: 39.23%, Valid: 42.28% Test: 40.64%\n",
            "Epoch: 48, Loss: 2.5902, Train: 39.44%, Valid: 42.51% Test: 40.89%\n",
            "Epoch: 49, Loss: 2.5815, Train: 39.64%, Valid: 42.74% Test: 41.08%\n",
            "Epoch: 50, Loss: 2.5708, Train: 39.85%, Valid: 42.92% Test: 41.32%\n",
            "Epoch: 51, Loss: 2.5624, Train: 40.07%, Valid: 43.13% Test: 41.56%\n",
            "Epoch: 52, Loss: 2.5517, Train: 40.29%, Valid: 43.34% Test: 41.76%\n",
            "Epoch: 53, Loss: 2.5428, Train: 40.49%, Valid: 43.56% Test: 41.96%\n",
            "Epoch: 54, Loss: 2.5356, Train: 40.70%, Valid: 43.76% Test: 42.17%\n",
            "Epoch: 55, Loss: 2.5238, Train: 40.90%, Valid: 43.96% Test: 42.36%\n",
            "Epoch: 56, Loss: 2.5178, Train: 41.09%, Valid: 44.16% Test: 42.59%\n",
            "Epoch: 57, Loss: 2.5079, Train: 41.28%, Valid: 44.38% Test: 42.74%\n",
            "Epoch: 58, Loss: 2.4999, Train: 41.45%, Valid: 44.51% Test: 42.89%\n",
            "Epoch: 59, Loss: 2.4902, Train: 41.62%, Valid: 44.73% Test: 43.08%\n",
            "Epoch: 60, Loss: 2.4828, Train: 41.84%, Valid: 44.87% Test: 43.28%\n",
            "Epoch: 61, Loss: 2.4735, Train: 42.01%, Valid: 45.05% Test: 43.49%\n",
            "Epoch: 62, Loss: 2.4646, Train: 42.20%, Valid: 45.25% Test: 43.66%\n",
            "Epoch: 63, Loss: 2.4597, Train: 42.39%, Valid: 45.39% Test: 43.85%\n",
            "Epoch: 64, Loss: 2.4526, Train: 42.59%, Valid: 45.54% Test: 44.00%\n",
            "Epoch: 65, Loss: 2.4439, Train: 42.76%, Valid: 45.72% Test: 44.13%\n",
            "Epoch: 66, Loss: 2.4362, Train: 42.94%, Valid: 45.84% Test: 44.31%\n",
            "Epoch: 67, Loss: 2.4325, Train: 43.08%, Valid: 45.99% Test: 44.44%\n",
            "Epoch: 68, Loss: 2.4248, Train: 43.27%, Valid: 46.13% Test: 44.52%\n",
            "Epoch: 69, Loss: 2.4159, Train: 43.43%, Valid: 46.25% Test: 44.63%\n",
            "Epoch: 70, Loss: 2.4087, Train: 43.58%, Valid: 46.35% Test: 44.77%\n",
            "Epoch: 71, Loss: 2.4052, Train: 43.73%, Valid: 46.44% Test: 44.86%\n",
            "Epoch: 72, Loss: 2.3915, Train: 43.86%, Valid: 46.61% Test: 44.97%\n",
            "Epoch: 73, Loss: 2.3901, Train: 43.99%, Valid: 46.74% Test: 45.07%\n",
            "Epoch: 74, Loss: 2.3838, Train: 44.17%, Valid: 46.93% Test: 45.16%\n",
            "Epoch: 75, Loss: 2.3740, Train: 44.33%, Valid: 47.03% Test: 45.28%\n",
            "Epoch: 76, Loss: 2.3728, Train: 44.47%, Valid: 47.12% Test: 45.39%\n",
            "Epoch: 77, Loss: 2.3670, Train: 44.58%, Valid: 47.25% Test: 45.49%\n",
            "Epoch: 78, Loss: 2.3634, Train: 44.70%, Valid: 47.38% Test: 45.61%\n",
            "Epoch: 79, Loss: 2.3541, Train: 44.83%, Valid: 47.46% Test: 45.70%\n",
            "Epoch: 80, Loss: 2.3512, Train: 44.97%, Valid: 47.56% Test: 45.79%\n",
            "Epoch: 81, Loss: 2.3440, Train: 45.06%, Valid: 47.67% Test: 45.88%\n",
            "Epoch: 82, Loss: 2.3351, Train: 45.21%, Valid: 47.77% Test: 45.96%\n",
            "Epoch: 83, Loss: 2.3361, Train: 45.33%, Valid: 47.87% Test: 46.03%\n",
            "Epoch: 84, Loss: 2.3275, Train: 45.45%, Valid: 47.95% Test: 46.09%\n",
            "Epoch: 85, Loss: 2.3269, Train: 45.52%, Valid: 48.00% Test: 46.17%\n",
            "Epoch: 86, Loss: 2.3174, Train: 45.61%, Valid: 48.08% Test: 46.17%\n",
            "Epoch: 87, Loss: 2.3140, Train: 45.71%, Valid: 48.15% Test: 46.26%\n",
            "Epoch: 88, Loss: 2.3100, Train: 45.79%, Valid: 48.22% Test: 46.32%\n",
            "Epoch: 89, Loss: 2.3053, Train: 45.89%, Valid: 48.29% Test: 46.40%\n",
            "Epoch: 90, Loss: 2.3002, Train: 45.96%, Valid: 48.35% Test: 46.49%\n",
            "Epoch: 91, Loss: 2.2963, Train: 46.06%, Valid: 48.44% Test: 46.55%\n",
            "Epoch: 92, Loss: 2.2939, Train: 46.17%, Valid: 48.59% Test: 46.59%\n",
            "Epoch: 93, Loss: 2.2840, Train: 46.25%, Valid: 48.64% Test: 46.62%\n",
            "Epoch: 94, Loss: 2.2803, Train: 46.35%, Valid: 48.78% Test: 46.68%\n",
            "Epoch: 95, Loss: 2.2766, Train: 46.44%, Valid: 48.86% Test: 46.75%\n",
            "Epoch: 96, Loss: 2.2770, Train: 46.54%, Valid: 48.93% Test: 46.81%\n",
            "Epoch: 97, Loss: 2.2722, Train: 46.63%, Valid: 48.97% Test: 46.88%\n",
            "Epoch: 98, Loss: 2.2715, Train: 46.73%, Valid: 49.04% Test: 46.98%\n",
            "Epoch: 99, Loss: 2.2625, Train: 46.80%, Valid: 49.12% Test: 47.09%\n",
            "Epoch: 100, Loss: 2.2589, Train: 46.90%, Valid: 49.18% Test: 47.16%\n"
          ]
        }
      ],
      "source": [
        "model = GCN_Classifier(input_dim=dataset.num_features,\n",
        "                       hidden_dim=args['hidden_dim'],\n",
        "                       output_dim=dataset.num_classes,\n",
        "                       dropout=args['dropout']).to(args['device'])\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "evaluator = Evaluator(name='ogbn-arxiv')\n",
        "loss_fn = nll_loss\n",
        "\n",
        "best_model = None\n",
        "best_valid_acc = 0\n",
        "\n",
        "for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "  loss = train(model, data, train_idx, args['trans_matrix'], optimizer, loss_fn)\n",
        "  result = test(model, data, split_idx, args['trans_matrix'], evaluator)\n",
        "  train_acc, valid_acc, test_acc = result\n",
        "  if valid_acc > best_valid_acc:\n",
        "      best_valid_acc = valid_acc\n",
        "      best_model = copy.deepcopy(model)\n",
        "  print(f'Epoch: {epoch:02d}, '\n",
        "        f'Loss: {loss:.4f}, '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7knZa2zd9dX"
      },
      "source": [
        "# Diffusion Heat"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## from Scratch "
      ],
      "metadata": {
        "id": "bZCW-xzAMmwp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "o_iqPK6geAGb"
      },
      "outputs": [],
      "source": [
        "def gdc_heat(A, t, sum_limit, eps):\n",
        "\n",
        "    N = A.shape[0]\n",
        "    # Self-loops\n",
        "    indices = torch.arange(N).unsqueeze(0).repeat(2, 1).to(device) \n",
        "    values = torch.ones(N, dtype=torch.float).to(device) \n",
        "    sparse_identiy = torch.sparse_coo_tensor(indices, values,\n",
        "                                             size=(N, N))    \n",
        "    \n",
        "    A_loop = A + sparse_identiy\n",
        "    \n",
        "    # Symmetric transition matrix\n",
        "    D_loop = torch.sparse.sum(A_loop, dim=1).to_dense()\n",
        "    D_sqrt_inv = torch.pow(D_loop, -0.5)\n",
        "    D_sqrt_inv = torch.sparse_coo_tensor(indices, D_sqrt_inv,\n",
        "                                         size=(N, N))\n",
        "\n",
        "    T_sym = D_sqrt_inv @ A_loop @ D_sqrt_inv\n",
        "\n",
        "    S = torch.sparse_coo_tensor(size=(N, N)).to(device)\n",
        "    T_k = sparse_identiy\n",
        "    \n",
        "    for k in range(sum_limit):\n",
        "      heat_coeff = math.exp(-t * t**k / math.factorial(k))\n",
        "      rlt = heat_coeff * T_k\n",
        "      T_sym= T_sym.coalesce()\n",
        "      T_sym = torch.sparse_coo_tensor(T_sym.indices(), torch.pow(T_sym.values(),k),  T_sym.shape) \n",
        "      S += rlt @ T_sym\n",
        "    \n",
        "    #T_k = T_k @ T_sym\n",
        "\n",
        "    # TODO : check why negative values are present in S\n",
        "    # Sparsify using threshold epsilon\n",
        "    indices =  S.coalesce().indices()\n",
        "    thresholded_val = S.coalesce().values() * (S.coalesce().values() >= eps)\n",
        "    S_tilde = torch.sparse_coo_tensor(indices, thresholded_val,\n",
        "                                      size=(N, N))\n",
        "\n",
        "    # Column-normalized transition matrix on graph S_tilde\n",
        "    D_tilde_vec = torch.sparse.sum(S_tilde, dim=1).to_dense()\n",
        "    indices = torch.arange(N).unsqueeze(0).repeat(2, 1).to(device) \n",
        "    D_tilde_vec = torch.sparse_coo_tensor(indices, D_tilde_vec,\n",
        "                                            size=(N, N))\n",
        "    T_S = S_tilde @ torch.pow(D_tilde_vec, -1)\n",
        "    \n",
        "    return T_S\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With pytorch function"
      ],
      "metadata": {
        "id": "JSKq-nEtNsj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.utils import (\n",
        "    add_self_loops,\n",
        "    coalesce,\n",
        "    is_undirected,\n",
        "    scatter,\n",
        "    to_dense_adj,)\n",
        "\n",
        "def expm(matrix) :\n",
        "  mat = matrix.coalesce()\n",
        "  diff_mat = torch.sparse_coo_tensor(mat.indices(), mat.values().exp(), mat.shape)      \n",
        "  #diff_mat_np = expm(matrix.cpu().numpy())\n",
        "  #diff_mat = torch.Tensor(diff_mat_np).to(matrix.device)\n",
        "  return diff_mat\n",
        "\n",
        "\n",
        "def gdc_heat2(A, t,k, sum_limit, eps):\n",
        "  num_nodes = A.shape[0]\n",
        "  indices = torch.arange(num_nodes).unsqueeze(0).repeat(2, 1).to(device) \n",
        "  values = torch.ones(num_nodes, dtype=torch.float).to(device) \n",
        "\n",
        "  edge_index, edge_weight = add_self_loops(indices, values,fill_value=-1, num_nodes=num_nodes)\n",
        "  edge_weight = t * edge_weight\n",
        "  mat = torch.sparse_coo_tensor(edge_index, edge_weight,size=(num_nodes, num_nodes))\n",
        "  print(mat)\n",
        "  #mat = to_dense_adj(edge_index, edge_attr=edge_weight).squeeze()\n",
        "  #undirected = is_undirected(edge_index, edge_weight, num_nodes)\n",
        "     \n",
        "  #diff_matrix = expm(mat, False)\n",
        "\n",
        "  return mat\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dXtgy206NEef"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "S = gdc_heat(A, 3, 25, 1e-4)"
      ],
      "metadata": {
        "id": "vNVE_N_wILdb"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "CeguVlH1kZHC"
      },
      "outputs": [],
      "source": [
        "args = {\n",
        "    'device': device,\n",
        "    'num_layers': 5,\n",
        "    'hidden_dim': 64,\n",
        "    'dropout': 0.5,\n",
        "    'lr': 0.001,\n",
        "    'epochs': 150,\n",
        "    'trans_matrix': S\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN_Classifier(input_dim=dataset.num_features,\n",
        "                       hidden_dim=args['hidden_dim'],\n",
        "                       output_dim=dataset.num_classes,\n",
        "                       dropout=args['dropout']).to(args['device'])\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "evaluator = Evaluator(name='ogbn-arxiv')\n",
        "loss_fn = nll_loss\n",
        "\n",
        "best_model = None\n",
        "best_valid_acc = 0\n",
        "\n",
        "for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "  loss = train(model, data, train_idx, args['trans_matrix'], optimizer, loss_fn)\n",
        "  result = test(model, data, split_idx, args['trans_matrix'], evaluator)\n",
        "  train_acc, valid_acc, test_acc = result\n",
        "  if valid_acc > best_valid_acc:\n",
        "      best_valid_acc = valid_acc\n",
        "      best_model = copy.deepcopy(model)\n",
        "  print(f'Epoch: {epoch:02d}, '\n",
        "        f'Loss: {loss:.4f}, '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxpfeIYDKfMR",
        "outputId": "51dce613-1470-4541-cae5-595c67dea327"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 3.7595, Train: 0.41%, Valid: 0.39% Test: 0.36%\n",
            "Epoch: 02, Loss: 3.6970, Train: 0.36%, Valid: 0.33% Test: 0.28%\n",
            "Epoch: 03, Loss: 3.6474, Train: 0.30%, Valid: 0.29% Test: 0.24%\n",
            "Epoch: 04, Loss: 3.6074, Train: 0.29%, Valid: 0.25% Test: 0.21%\n",
            "Epoch: 05, Loss: 3.5696, Train: 0.28%, Valid: 0.24% Test: 0.20%\n",
            "Epoch: 06, Loss: 3.5377, Train: 0.31%, Valid: 0.30% Test: 0.25%\n",
            "Epoch: 07, Loss: 3.5049, Train: 0.62%, Valid: 0.58% Test: 0.44%\n",
            "Epoch: 08, Loss: 3.4704, Train: 1.46%, Valid: 1.27% Test: 0.93%\n",
            "Epoch: 09, Loss: 3.4391, Train: 2.84%, Valid: 2.22% Test: 1.73%\n",
            "Epoch: 10, Loss: 3.4107, Train: 4.13%, Valid: 2.97% Test: 2.34%\n",
            "Epoch: 11, Loss: 3.3792, Train: 4.76%, Valid: 3.31% Test: 2.57%\n",
            "Epoch: 12, Loss: 3.3478, Train: 5.06%, Valid: 3.48% Test: 2.70%\n",
            "Epoch: 13, Loss: 3.3199, Train: 5.63%, Valid: 3.80% Test: 2.83%\n",
            "Epoch: 14, Loss: 3.2887, Train: 6.37%, Valid: 4.32% Test: 3.02%\n",
            "Epoch: 15, Loss: 3.2617, Train: 7.25%, Valid: 5.15% Test: 3.37%\n",
            "Epoch: 16, Loss: 3.2320, Train: 8.29%, Valid: 6.20% Test: 3.96%\n",
            "Epoch: 17, Loss: 3.2064, Train: 9.39%, Valid: 7.26% Test: 4.78%\n",
            "Epoch: 18, Loss: 3.1790, Train: 10.69%, Valid: 8.28% Test: 5.59%\n",
            "Epoch: 19, Loss: 3.1473, Train: 12.01%, Valid: 9.21% Test: 6.31%\n",
            "Epoch: 20, Loss: 3.1196, Train: 13.51%, Valid: 10.32% Test: 7.29%\n",
            "Epoch: 21, Loss: 3.0891, Train: 15.25%, Valid: 11.81% Test: 8.77%\n",
            "Epoch: 22, Loss: 3.0687, Train: 17.07%, Valid: 13.73% Test: 10.78%\n",
            "Epoch: 23, Loss: 3.0419, Train: 18.93%, Valid: 15.83% Test: 13.17%\n",
            "Epoch: 24, Loss: 3.0149, Train: 20.90%, Valid: 17.83% Test: 15.70%\n",
            "Epoch: 25, Loss: 2.9951, Train: 22.79%, Valid: 19.98% Test: 18.27%\n",
            "Epoch: 26, Loss: 2.9735, Train: 24.65%, Valid: 22.12% Test: 21.10%\n",
            "Epoch: 27, Loss: 2.9468, Train: 26.34%, Valid: 24.05% Test: 23.65%\n",
            "Epoch: 28, Loss: 2.9241, Train: 27.85%, Valid: 25.70% Test: 25.95%\n",
            "Epoch: 29, Loss: 2.9048, Train: 29.33%, Valid: 27.08% Test: 27.96%\n",
            "Epoch: 30, Loss: 2.8840, Train: 31.08%, Valid: 28.99% Test: 30.32%\n",
            "Epoch: 31, Loss: 2.8641, Train: 33.01%, Valid: 31.40% Test: 33.18%\n",
            "Epoch: 32, Loss: 2.8461, Train: 34.89%, Valid: 33.48% Test: 35.86%\n",
            "Epoch: 33, Loss: 2.8253, Train: 36.42%, Valid: 35.40% Test: 38.10%\n",
            "Epoch: 34, Loss: 2.8053, Train: 37.89%, Valid: 37.25% Test: 40.07%\n",
            "Epoch: 35, Loss: 2.7872, Train: 39.30%, Valid: 39.07% Test: 42.02%\n",
            "Epoch: 36, Loss: 2.7651, Train: 40.73%, Valid: 41.25% Test: 43.86%\n",
            "Epoch: 37, Loss: 2.7480, Train: 42.00%, Valid: 43.04% Test: 45.35%\n",
            "Epoch: 38, Loss: 2.7287, Train: 43.03%, Valid: 44.36% Test: 46.38%\n",
            "Epoch: 39, Loss: 2.7081, Train: 43.94%, Valid: 45.26% Test: 47.13%\n",
            "Epoch: 40, Loss: 2.6953, Train: 44.79%, Valid: 46.20% Test: 48.11%\n",
            "Epoch: 41, Loss: 2.6756, Train: 45.51%, Valid: 47.29% Test: 49.21%\n",
            "Epoch: 42, Loss: 2.6571, Train: 46.36%, Valid: 48.49% Test: 50.53%\n",
            "Epoch: 43, Loss: 2.6449, Train: 47.17%, Valid: 49.74% Test: 51.90%\n",
            "Epoch: 44, Loss: 2.6288, Train: 47.92%, Valid: 50.73% Test: 52.88%\n",
            "Epoch: 45, Loss: 2.6140, Train: 48.65%, Valid: 51.51% Test: 53.73%\n",
            "Epoch: 46, Loss: 2.5979, Train: 49.35%, Valid: 52.21% Test: 54.34%\n",
            "Epoch: 47, Loss: 2.5831, Train: 50.03%, Valid: 52.82% Test: 54.90%\n",
            "Epoch: 48, Loss: 2.5679, Train: 50.57%, Valid: 53.43% Test: 55.50%\n",
            "Epoch: 49, Loss: 2.5570, Train: 51.05%, Valid: 54.09% Test: 56.19%\n",
            "Epoch: 50, Loss: 2.5384, Train: 51.47%, Valid: 54.59% Test: 56.78%\n",
            "Epoch: 51, Loss: 2.5263, Train: 51.83%, Valid: 54.98% Test: 57.11%\n",
            "Epoch: 52, Loss: 2.5133, Train: 52.12%, Valid: 55.41% Test: 57.47%\n",
            "Epoch: 53, Loss: 2.4965, Train: 52.48%, Valid: 55.64% Test: 57.77%\n",
            "Epoch: 54, Loss: 2.4796, Train: 52.69%, Valid: 55.77% Test: 57.92%\n",
            "Epoch: 55, Loss: 2.4697, Train: 53.01%, Valid: 55.90% Test: 58.05%\n",
            "Epoch: 56, Loss: 2.4535, Train: 53.35%, Valid: 56.14% Test: 58.31%\n",
            "Epoch: 57, Loss: 2.4451, Train: 53.64%, Valid: 56.61% Test: 58.65%\n",
            "Epoch: 58, Loss: 2.4324, Train: 53.92%, Valid: 56.83% Test: 58.72%\n",
            "Epoch: 59, Loss: 2.4181, Train: 54.15%, Valid: 56.87% Test: 58.80%\n",
            "Epoch: 60, Loss: 2.4039, Train: 54.31%, Valid: 56.84% Test: 58.82%\n",
            "Epoch: 61, Loss: 2.3942, Train: 54.54%, Valid: 56.98% Test: 58.93%\n",
            "Epoch: 62, Loss: 2.3791, Train: 54.92%, Valid: 57.35% Test: 59.30%\n",
            "Epoch: 63, Loss: 2.3687, Train: 55.19%, Valid: 57.73% Test: 59.48%\n",
            "Epoch: 64, Loss: 2.3570, Train: 55.45%, Valid: 57.97% Test: 59.57%\n",
            "Epoch: 65, Loss: 2.3458, Train: 55.75%, Valid: 58.19% Test: 59.77%\n",
            "Epoch: 66, Loss: 2.3331, Train: 56.08%, Valid: 58.54% Test: 60.07%\n",
            "Epoch: 67, Loss: 2.3221, Train: 56.33%, Valid: 58.71% Test: 60.23%\n",
            "Epoch: 68, Loss: 2.3119, Train: 56.59%, Valid: 58.94% Test: 60.33%\n",
            "Epoch: 69, Loss: 2.2999, Train: 56.84%, Valid: 59.14% Test: 60.55%\n",
            "Epoch: 70, Loss: 2.2884, Train: 57.11%, Valid: 59.60% Test: 60.80%\n",
            "Epoch: 71, Loss: 2.2790, Train: 57.41%, Valid: 59.88% Test: 61.03%\n",
            "Epoch: 72, Loss: 2.2693, Train: 57.60%, Valid: 59.94% Test: 61.04%\n",
            "Epoch: 73, Loss: 2.2585, Train: 57.90%, Valid: 60.13% Test: 61.12%\n",
            "Epoch: 74, Loss: 2.2478, Train: 58.27%, Valid: 60.40% Test: 61.44%\n",
            "Epoch: 75, Loss: 2.2388, Train: 58.54%, Valid: 60.68% Test: 61.68%\n",
            "Epoch: 76, Loss: 2.2293, Train: 58.70%, Valid: 60.82% Test: 61.60%\n",
            "Epoch: 77, Loss: 2.2177, Train: 58.83%, Valid: 60.92% Test: 61.62%\n",
            "Epoch: 78, Loss: 2.2076, Train: 59.02%, Valid: 61.11% Test: 61.80%\n",
            "Epoch: 79, Loss: 2.2000, Train: 59.25%, Valid: 61.33% Test: 61.96%\n",
            "Epoch: 80, Loss: 2.1930, Train: 59.50%, Valid: 61.46% Test: 62.17%\n",
            "Epoch: 81, Loss: 2.1844, Train: 59.61%, Valid: 61.45% Test: 62.30%\n",
            "Epoch: 82, Loss: 2.1752, Train: 59.67%, Valid: 61.56% Test: 62.36%\n",
            "Epoch: 83, Loss: 2.1648, Train: 59.89%, Valid: 61.76% Test: 62.58%\n",
            "Epoch: 84, Loss: 2.1580, Train: 60.14%, Valid: 62.01% Test: 62.64%\n",
            "Epoch: 85, Loss: 2.1470, Train: 60.29%, Valid: 62.19% Test: 62.56%\n",
            "Epoch: 86, Loss: 2.1376, Train: 60.35%, Valid: 62.03% Test: 62.41%\n",
            "Epoch: 87, Loss: 2.1310, Train: 60.40%, Valid: 62.03% Test: 62.42%\n",
            "Epoch: 88, Loss: 2.1250, Train: 60.55%, Valid: 62.23% Test: 62.78%\n",
            "Epoch: 89, Loss: 2.1180, Train: 60.74%, Valid: 62.48% Test: 63.13%\n",
            "Epoch: 90, Loss: 2.1062, Train: 60.85%, Valid: 62.62% Test: 63.19%\n",
            "Epoch: 91, Loss: 2.0997, Train: 60.96%, Valid: 62.71% Test: 63.04%\n",
            "Epoch: 92, Loss: 2.0921, Train: 61.03%, Valid: 62.61% Test: 63.05%\n",
            "Epoch: 93, Loss: 2.0826, Train: 61.14%, Valid: 62.75% Test: 63.20%\n",
            "Epoch: 94, Loss: 2.0746, Train: 61.21%, Valid: 62.82% Test: 63.28%\n",
            "Epoch: 95, Loss: 2.0681, Train: 61.32%, Valid: 62.80% Test: 63.18%\n",
            "Epoch: 96, Loss: 2.0640, Train: 61.41%, Valid: 62.77% Test: 62.98%\n",
            "Epoch: 97, Loss: 2.0534, Train: 61.58%, Valid: 63.03% Test: 63.18%\n",
            "Epoch: 98, Loss: 2.0449, Train: 61.67%, Valid: 63.23% Test: 63.25%\n",
            "Epoch: 99, Loss: 2.0413, Train: 61.70%, Valid: 63.16% Test: 63.27%\n",
            "Epoch: 100, Loss: 2.0388, Train: 61.67%, Valid: 63.11% Test: 63.34%\n",
            "Epoch: 101, Loss: 2.0275, Train: 61.77%, Valid: 63.31% Test: 63.63%\n",
            "Epoch: 102, Loss: 2.0191, Train: 61.85%, Valid: 63.55% Test: 63.86%\n",
            "Epoch: 103, Loss: 2.0121, Train: 61.88%, Valid: 63.60% Test: 63.95%\n",
            "Epoch: 104, Loss: 2.0053, Train: 61.88%, Valid: 63.49% Test: 63.71%\n",
            "Epoch: 105, Loss: 2.0009, Train: 62.11%, Valid: 63.52% Test: 63.65%\n",
            "Epoch: 106, Loss: 1.9923, Train: 62.30%, Valid: 63.79% Test: 63.80%\n",
            "Epoch: 107, Loss: 1.9845, Train: 62.37%, Valid: 63.89% Test: 64.07%\n",
            "Epoch: 108, Loss: 1.9808, Train: 62.40%, Valid: 63.86% Test: 64.03%\n",
            "Epoch: 109, Loss: 1.9768, Train: 62.42%, Valid: 63.81% Test: 64.00%\n",
            "Epoch: 110, Loss: 1.9644, Train: 62.52%, Valid: 63.92% Test: 64.07%\n",
            "Epoch: 111, Loss: 1.9659, Train: 62.55%, Valid: 64.03% Test: 64.31%\n",
            "Epoch: 112, Loss: 1.9605, Train: 62.57%, Valid: 64.12% Test: 64.34%\n",
            "Epoch: 113, Loss: 1.9513, Train: 62.79%, Valid: 64.22% Test: 64.26%\n",
            "Epoch: 114, Loss: 1.9425, Train: 62.99%, Valid: 64.24% Test: 64.09%\n",
            "Epoch: 115, Loss: 1.9439, Train: 63.04%, Valid: 64.31% Test: 64.15%\n",
            "Epoch: 116, Loss: 1.9402, Train: 63.02%, Valid: 64.29% Test: 64.29%\n",
            "Epoch: 117, Loss: 1.9321, Train: 62.99%, Valid: 64.37% Test: 64.46%\n",
            "Epoch: 118, Loss: 1.9251, Train: 63.02%, Valid: 64.46% Test: 64.59%\n",
            "Epoch: 119, Loss: 1.9201, Train: 63.16%, Valid: 64.53% Test: 64.58%\n",
            "Epoch: 120, Loss: 1.9118, Train: 63.18%, Valid: 64.49% Test: 64.51%\n",
            "Epoch: 121, Loss: 1.9101, Train: 63.17%, Valid: 64.46% Test: 64.53%\n",
            "Epoch: 122, Loss: 1.9027, Train: 63.19%, Valid: 64.47% Test: 64.53%\n",
            "Epoch: 123, Loss: 1.8992, Train: 63.39%, Valid: 64.70% Test: 64.71%\n",
            "Epoch: 124, Loss: 1.8930, Train: 63.50%, Valid: 64.80% Test: 64.79%\n",
            "Epoch: 125, Loss: 1.8889, Train: 63.44%, Valid: 64.80% Test: 64.80%\n",
            "Epoch: 126, Loss: 1.8821, Train: 63.31%, Valid: 64.63% Test: 64.79%\n",
            "Epoch: 127, Loss: 1.8781, Train: 63.44%, Valid: 64.75% Test: 64.79%\n",
            "Epoch: 128, Loss: 1.8697, Train: 63.70%, Valid: 64.90% Test: 64.76%\n",
            "Epoch: 129, Loss: 1.8636, Train: 63.82%, Valid: 64.80% Test: 64.55%\n",
            "Epoch: 130, Loss: 1.8609, Train: 63.84%, Valid: 64.78% Test: 64.53%\n",
            "Epoch: 131, Loss: 1.8602, Train: 63.76%, Valid: 64.82% Test: 64.82%\n",
            "Epoch: 132, Loss: 1.8499, Train: 63.71%, Valid: 64.99% Test: 65.17%\n",
            "Epoch: 133, Loss: 1.8512, Train: 63.79%, Valid: 65.08% Test: 65.28%\n",
            "Epoch: 134, Loss: 1.8454, Train: 63.94%, Valid: 65.14% Test: 65.19%\n",
            "Epoch: 135, Loss: 1.8381, Train: 64.05%, Valid: 65.08% Test: 64.86%\n",
            "Epoch: 136, Loss: 1.8357, Train: 64.11%, Valid: 65.17% Test: 64.84%\n",
            "Epoch: 137, Loss: 1.8279, Train: 64.13%, Valid: 65.34% Test: 65.16%\n",
            "Epoch: 138, Loss: 1.8271, Train: 64.13%, Valid: 65.38% Test: 65.35%\n",
            "Epoch: 139, Loss: 1.8275, Train: 64.11%, Valid: 65.24% Test: 65.31%\n",
            "Epoch: 140, Loss: 1.8226, Train: 64.10%, Valid: 65.21% Test: 65.24%\n",
            "Epoch: 141, Loss: 1.8144, Train: 64.23%, Valid: 65.23% Test: 65.15%\n",
            "Epoch: 142, Loss: 1.8099, Train: 64.42%, Valid: 65.33% Test: 65.13%\n",
            "Epoch: 143, Loss: 1.8104, Train: 64.44%, Valid: 65.34% Test: 65.28%\n",
            "Epoch: 144, Loss: 1.7998, Train: 64.48%, Valid: 65.45% Test: 65.33%\n",
            "Epoch: 145, Loss: 1.7965, Train: 64.54%, Valid: 65.53% Test: 65.33%\n",
            "Epoch: 146, Loss: 1.7978, Train: 64.61%, Valid: 65.49% Test: 65.15%\n",
            "Epoch: 147, Loss: 1.7944, Train: 64.62%, Valid: 65.42% Test: 65.08%\n",
            "Epoch: 148, Loss: 1.7866, Train: 64.71%, Valid: 65.65% Test: 65.23%\n",
            "Epoch: 149, Loss: 1.7852, Train: 64.71%, Valid: 65.63% Test: 65.36%\n",
            "Epoch: 150, Loss: 1.7809, Train: 64.57%, Valid: 65.49% Test: 65.45%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BGcxZqVSKqBT"
      },
      "execution_count": 17,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "877977f2f616ef89ae9c0db206424d48530d28929fbeef55383760714580946d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}